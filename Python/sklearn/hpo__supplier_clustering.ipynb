{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1 style=\"color: #555555;\">Experience Script Documentation</h1></center>\n",
    "\n",
    "<div align=\"center\">\n",
    "    <table style=\"width: 80%; border-collapse: collapse;\">\n",
    "        <tr>\n",
    "            <th style=\"background-color: #6AAFE6; color: #ffffff; padding: 10px;\">Context</th>\n",
    "            <th style=\"background-color: #6AAFE6; color: #ffffff; padding: 10px;\">Approach</th>\n",
    "            <th style=\"background-color: #6AAFE6; color: #ffffff; padding: 10px;\">Value Created</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"background-color: #E8F4FC; padding: 10px;\">Provide the background and purpose of the script.</td>\n",
    "            <td style=\"background-color: #E8F4FC; padding: 10px;\">Describe the libraries, methods, and thought process.</td>\n",
    "            <td style=\"background-color: #E8F4FC; padding: 10px;\">Highlight the outcomes, improvements, and conclusions.</td>\n",
    "        </tr>\n",
    "    </table>\n",
    "</div>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dtype = {\n",
    "    'SUPPLIER_ERP': 'str', \n",
    "    'SUPPLIER_NORMALIZED': 'str',\n",
    "}\n",
    "\n",
    "#invoice_data = pd.read_csv('sap supplier list apr-jun23.csv', encoding='UTF-8-SIG', dtype=dtype)\n",
    "df = pd.read_csv('hpo supplier list dec23 v.1.csv', encoding='UTF-8-SIG', dtype=dtype)\n",
    "\n",
    "#Use .loc to strip whitespaces in 'SUPPLIER_ERP' and 'SUPPLIER_NORMALIZED'\n",
    "df.loc[:, 'SUPPLIER_ERP'] = df['SUPPLIER_ERP'].str.strip()\n",
    "df.loc[:, 'SUPPLIER_NORMALIZED'] = df['SUPPLIER_NORMALIZED'].str.strip()\n",
    "\n",
    "df = df.dropna(subset=['SUPPLIER_ERP', 'SUPPLIER_NORMALIZED'], how='any', inplace=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom TF-IDF vectorizer that boosts the TF-IDF scores of the words of interest\n",
    "\n",
    "\n",
    "class CustomTfidfVectorizer(TfidfVectorizer):\n",
    "    def __init__(self, keyword_list, boost_val, \n",
    "                 input='content', encoding='utf-8', decode_error='strict', \n",
    "                 strip_accents=None, lowercase=False, preprocessor=None, \n",
    "                 tokenizer=None, analyzer='word', stop_words=None, \n",
    "                 token_pattern=r\"(?u)\\b\\w\\w+\\b\", ngram_range=(1, 1), \n",
    "                 max_df=1.0, min_df=1, max_features=None, vocabulary=None, \n",
    "                 binary=False, dtype=np.int64, norm='l2', use_idf=True, \n",
    "                 smooth_idf=True, sublinear_tf=False):\n",
    "\n",
    "        # Initialize our added parameters\n",
    "        self.keyword_list = keyword_list\n",
    "        self.boost_val = boost_val\n",
    "\n",
    "        # Initialize TfidfVectorizer with passed parameters\n",
    "        super().__init__(input=input, encoding=encoding, decode_error=decode_error, \n",
    "                         strip_accents=strip_accents, lowercase=lowercase, preprocessor=preprocessor, \n",
    "                         tokenizer=tokenizer, analyzer=analyzer, stop_words=stop_words, \n",
    "                         token_pattern=token_pattern, ngram_range=ngram_range, \n",
    "                         max_df=max_df, min_df=min_df, max_features=max_features, vocabulary=vocabulary, \n",
    "                         binary=binary, dtype=dtype, norm=norm, use_idf=use_idf, \n",
    "                         smooth_idf=smooth_idf, sublinear_tf=sublinear_tf)\n",
    "\n",
    "def transform(self, raw_documents, copy=True):\n",
    "    X = super(CustomTfidfVectorizer, self).transform(raw_documents, copy=copy)\n",
    "    for word in self.keyword_list:\n",
    "        if word in self.vocabulary_:\n",
    "            X[:, self.vocabulary_[word]] *= self.boost_val\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keywords to emphasize clustering\n",
    "keywords = [\"AMAZON\", \"BJS WHOLESALE CLUB\",\"BRUEGGERS\", \"DUNKIN DONUTS\", \"MICROSOFT\", \"SAMS CLUB\", \"TWILIO\", \"UBER\"\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\diana.fernandez\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2065: UserWarning: Only (<class 'numpy.float64'>, <class 'numpy.float32'>, <class 'numpy.float16'>) 'dtype' should be used. <class 'numpy.int64'> 'dtype' will be converted to np.float64.\n",
      "  warnings.warn(\n",
      "d:\\Users\\diana.fernandez\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    }
   ],
   "source": [
    "# Create a custom TF-IDF vectorizer\n",
    "vectorizer = CustomTfidfVectorizer(keyword_list=keywords, boost_val=8.0, stop_words=\"english\")\n",
    "\n",
    "# Fit the pipeline on the company names and compute the tfidf_matrix\n",
    "tfidf_matrix = vectorizer.fit_transform(df [\"SUPPLIER_NORMALIZED\"])\n",
    "\n",
    "# Transform the keywords into the TF-IDF space\n",
    "keywords_tfidf = vectorizer.transform(keywords)\n",
    "\n",
    "# Create a KMeans clustering model.\n",
    "n_clusters = 80  # Adjust this as per your requirements.\n",
    "kmeans = KMeans(n_clusters=n_clusters)\n",
    "\n",
    "# Cluster the data\n",
    "kmeans.fit(tfidf_matrix)\n",
    "\n",
    "# Get cluster labels\n",
    "labels = kmeans.labels_\n",
    "\n",
    "# Add the cluster labels back to the main dataframe\n",
    "df ['label'] = labels\n",
    "\n",
    "# Store cosine similarities to the centroid of each cluster\n",
    "similarities_to_centroid = []\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "for i, label in enumerate(labels):                                                                                                  \n",
    "    similarity = cosine_similarity(tfidf_matrix[i].reshape(1, -1), centroids[label].reshape(1, -1))[0][0]\n",
    "    similarities_to_centroid.append(similarity)\n",
    "\n",
    "df ['similarity'] = similarities_to_centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\diana.fernandez\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Inertia: 28.22430205932443\n"
     ]
    }
   ],
   "source": [
    "# Existing KMeans clustering part\n",
    "kmeans = KMeans(n_clusters=n_clusters)\n",
    "kmeans.fit(tfidf_matrix)\n",
    "                                      \n",
    "# Inertia        \n",
    "total_inertia = kmeans.inertia_\n",
    "print(f\"Total Inertia: {total_inertia}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the representative name for each cluster centroid\n",
    "representative_names = []\n",
    "for center in centroids:\n",
    "    # Check if centroid is close to a keyword vector\n",
    "    keyword_similarities = cosine_similarity(center.reshape(1, -1), keywords_tfidf)\n",
    "    max_keyword_similarity_index = keyword_similarities.argmax()\n",
    "    \n",
    "    # If the max similarity is above a certain threshold, use the keyword\n",
    "    if keyword_similarities[0, max_keyword_similarity_index] > 0.75:  # Adjust the threshold as needed\n",
    "        representative_name = keywords[max_keyword_similarity_index]\n",
    "    else:\n",
    "        # Find the closest supplier to the centroid\n",
    "        supplier_similarities = cosine_similarity(center.reshape(1, -1), tfidf_matrix)\n",
    "        representative_index = supplier_similarities.argmax()\n",
    "        representative_name = df .iloc[representative_index][\"SUPPLIER_NORMALIZED\"]\n",
    "    \n",
    "    representative_names.append(representative_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for center in centroids:\n",
    "    # Check if centroid is close to a keyword vector\n",
    "    keyword_similarities = cosine_similarity(center.reshape(1, -1), keywords_tfidf)\n",
    "    max_keyword_similarity_index = keyword_similarities.argmax()\n",
    "    \n",
    "    # If the max similarity is above a certain threshold, use the keyword\n",
    "    if keyword_similarities[0, max_keyword_similarity_index] > 0.75:  # Adjust the threshold as needed\n",
    "        representative_name = keywords[max_keyword_similarity_index]\n",
    "    else:\n",
    "        # Find the closest supplier to the centroid\n",
    "        supplier_similarities = cosine_similarity(center.reshape(1, -1), tfidf_matrix)\n",
    "        representative_index = supplier_similarities.argmax()\n",
    "        representative_name = df .iloc[representative_index][\"SUPPLIER_NORMALIZED\"]\n",
    "    \n",
    "    representative_names.append(representative_name)\n",
    "# Map cluster labels to the representative names\n",
    "cluster_to_name_map = {i: name for i, name in enumerate(representative_names)}\n",
    "\n",
    "# Add a new column to your dataframe for the representative names of each cluster\n",
    "df ['normalized_name'] = df ['label'].map(cluster_to_name_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the resulting table\n",
    "df[['label','FILE_NAME', 'SOURCE_DATA', 'SUPPLIER_ERP', 'SUPPLIER_NORMALIZED', 'normalized_name', 'SPEND_USD', 'similarity']].to_csv('hpo_clustered_suppliers_dec v.1.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
